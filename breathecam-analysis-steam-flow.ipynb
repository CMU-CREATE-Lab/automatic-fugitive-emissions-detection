{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pybgs as bgs\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from breathecam import BreatheCam\n",
    "from common import get_previous_frame_time\n",
    "from components import View\n",
    "from motion import temporal_events, mask_background, get_event\n",
    "\n",
    "\n",
    "def display_event(video, flattened=False):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ims = []\n",
    "\n",
    "    if len(video.shape) < 4 and flattened:\n",
    "        ims.append([ax.imshow(video)])\n",
    "    else:\n",
    "        for i in range(len(video)):\n",
    "            ims.append([ax.imshow(video[i], animated=True)])\n",
    "\n",
    "    anim = animation.ArtistAnimation(fig, ims, interval=175, blit=True, repeat_delay=1000)\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a8a63d6e2bb3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 4507x601 (216696560 pixels)\n"
     ]
    }
   ],
   "source": [
    "#expertiment settings and avoid midnight\n",
    "day = datetime.date.fromisoformat(\"2024-05-19\")\n",
    "time = datetime.time.fromisoformat(\"09:49:00\")\n",
    "previous_frame_time = get_previous_frame_time(time, 3)\n",
    "nframes = 80\n",
    "nlevels = 4\n",
    "view = View(2307, 1914, 6814, 2515)\n",
    "\n",
    "print(f\"Size: {view.width}x{view.height} ({view.width * view.height * nframes} pixels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5c1e77aa8e8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = BreatheCam.init_from(\"Clairton Coke Works\", day)\n",
    "breathecam_video = camera.download_video(previous_frame_time, nframes+1, view, nlevels)\n",
    "fullres_video = camera.download_video(time, nframes, view, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff50861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+PixelBasedAdaptiveSegmenter()\n",
      " PixelBasedAdaptiveSegmenter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to open ./config/PixelBasedAdaptiveSegmenter.xml\n",
      "Failed to open ./config/PixelBasedAdaptiveSegmenter.xml\n"
     ]
    }
   ],
   "source": [
    "#modified mask_background from motion.py so that can take bgs background subtractor instances\n",
    "def mask_background(video: np.ndarray, background_subtractor) -> np.ndarray:\n",
    "    \"\"\"Generates, by background subtraction, a video of foreground masks\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    * video - a collection of frames\n",
    "    * background_subtractor - a mask generating function to apply to each frame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    A collection of frames representing foreground objects in each frame\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(background_subtractor, cv2.BackgroundSubtractor):\n",
    "        print(\"using cv background subtractor...\")\n",
    "        f = background_subtractor.apply\n",
    "    elif isinstance(background_subtractor, bgs.SuBSENSE):\n",
    "        print(\"using bgs SuBENSE background subtractor\")\n",
    "        f = background_subtractor.apply\n",
    "    elif isinstance(background_subtractor, bgs.PixelBasedAdaptiveSegmenter):\n",
    "        print(\" PixelBasedAdaptiveSegmenter\")\n",
    "        f = background_subtractor.apply\n",
    "    elif isinstance(background_subtractor, bgs.LOBSTER):\n",
    "        print(\"LOBSTER\")\n",
    "        f = background_subtractor.apply\n",
    "    else:\n",
    "        #Also tried ViBe algorithm but couldn't find name\n",
    "        print(\"couldn't find name of bgs background subtractor class\")\n",
    "        f = background_subtractor.apply\n",
    "\n",
    "    if len(video.shape) == 3:\n",
    "        return np.array([f(video)])\n",
    "    else:\n",
    "        return np.array([f(v) for v in video])\n",
    "\n",
    "#instantiate background_subtractor instance here\n",
    "#name of class sometimes hard to find\n",
    "#find in this GitHub: https://github.com/andrewssobral/bgslibrary/tree/master/bgslibrary/algorithms\n",
    "background_subtractor = bgs.PixelBasedAdaptiveSegmenter()\n",
    "masks = mask_background(breathecam_video[1:], background_subtractor)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4efba8495b71f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eventspace = temporal_events(masks, neighbors=8, depth=3, threshold=127)\n",
    "event_filter = lambda e : e.number_of_frames > 3 and e.region.height > 10 and e.region.width > 10\n",
    "eventspace = list(filter(event_filter, eventspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3470d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more of a type than a class. Acutall functionality implemented in sub-classes\n",
    "\n",
    "#init method. set nessisary hyperparameters alpha, beta etc... \n",
    "\n",
    "#update method. mutates the passed estimates array. \n",
    "class Estimator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def update(self):\n",
    "        pass\n",
    "\n",
    "class Exponential_Smoothing_Estimator():\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def update(self, estimates, selected_pixels):\n",
    "        print(estimates.shape)\n",
    "        print(\"pixels\", selected_pixels.shape)\n",
    "        assert(estimates.shape == selected_pixels.shape)\n",
    "        rows, cols = estimates.shape\n",
    "\n",
    "        for r in rows:\n",
    "            for c in cols:\n",
    "                alpha_estimate = self.alpha * selected_pixels[r, c]\n",
    "                prev_estimate = (1 - self.alpha) * estimates[r, c]\n",
    "                estimates[r, c] = alpha_estimate + prev_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eac6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_steam_approx(masks, video, estimator, steam_probability, intensity_tresh = 70):\n",
    "    #Non-mutating. Returns a copy of video with steam pixels subtracted\n",
    "    \n",
    "    estimates = np.full(video.shape[1:3], 0.5)\n",
    "    for frameIdx in range(video.shape[0]):\n",
    "        # ########## THis block is buggy ########\n",
    "        # gray_frame = cv2.cvtColor(video[frameIdx], cv2.COLOR_BGR2GRAY)\n",
    "        # curr_mask = masks[frameIdx]\n",
    "        # selected_pixels = gray_frame[curr_mask == ]\n",
    "        # print('selected before before tresh', selected_pixels.shape)\n",
    "        # selected_pixels = selected_pixels > intensity_tresh\n",
    "        # print('selected before passS', selected_pixels.shape)\n",
    "        # ######end of buggy block ##########\n",
    "\n",
    "        # supposed to gray the frame \n",
    "        # extract pixels corrisponding to mask \n",
    "        #apply threshold condition \n",
    "\n",
    "\n",
    "\n",
    "        estimator.update(estimates, selected_pixels)\n",
    "    \n",
    "    #the following np code may also be buggy\n",
    "    #Remove any pixels with a high probability of being steam\n",
    "    steam_mask = estimates > steam_probability\n",
    "    non_steam_mask = not steam_mask\n",
    "    non_steam_mask = np.reshape(1, steam_mask.shape[0], steam_mask.shape[1], 1)\n",
    "    non_steam_video = video.copy()\n",
    "    non_steam_video[non_steam_mask] = 0\n",
    "    return non_steam_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f311fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected before before tresh (0,)\n",
      "selected before passS (0,)\n",
      "(151, 1127)\n",
      "pixels (0,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#TODO fix number of frames in video and masks \u001b[39;00m\n\u001b[32m      2\u001b[39m estimator = Exponential_Smoothing_Estimator(\u001b[32m0.1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m non_steam_video = \u001b[43mnaive_steam_approx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreathecam_video\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mnaive_steam_approx\u001b[39m\u001b[34m(masks, video, estimator, steam_probability, intensity_tresh)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mselected before passS\u001b[39m\u001b[33m'\u001b[39m, selected_pixels.shape)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m#slected pixels, estimates\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_pixels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#Remove any pixels with a high probability of being steam\u001b[39;00m\n\u001b[32m     16\u001b[39m steam_mask = estimates > steam_probability\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mExponential_Smoothing_Estimator.update\u001b[39m\u001b[34m(self, estimates, selected_pixels)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(estimates.shape)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpixels\u001b[39m\u001b[33m\"\u001b[39m, selected_pixels.shape)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m(estimates.shape == selected_pixels.shape)\n\u001b[32m     20\u001b[39m rows, cols = estimates.shape\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#TODO fix number of frames in video and masks \n",
    "estimator = Exponential_Smoothing_Estimator(0.1)\n",
    "non_steam_video = naive_steam_approx(masks, breathecam_video[1:], estimator, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_event(non_steam_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b482e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 79 is out of bounds for axis 0 with size 79",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m events_cropped_vids = [] \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     event = \u001b[43mget_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43meventspace\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullres_video\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     events_cropped_vids.append(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/breathCam/motion.py:59\u001b[39m, in \u001b[36mget_event\u001b[39m\u001b[34m(event, video, nlevels, pad_frames, pad_region)\u001b[39m\n\u001b[32m     56\u001b[39m end_frame = \u001b[38;5;28mmin\u001b[39m(event.frames[-\u001b[32m1\u001b[39m] + frame_end_padding, n - \u001b[32m1\u001b[39m)\n\u001b[32m     57\u001b[39m frames = [*\u001b[38;5;28mrange\u001b[39m(start_frame, event.frames[\u001b[32m0\u001b[39m]), *event.frames, *\u001b[38;5;28mrange\u001b[39m(n, end_frame + \u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array(\u001b[43m[\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/breathCam/motion.py:59\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     56\u001b[39m end_frame = \u001b[38;5;28mmin\u001b[39m(event.frames[-\u001b[32m1\u001b[39m] + frame_end_padding, n - \u001b[32m1\u001b[39m)\n\u001b[32m     57\u001b[39m frames = [*\u001b[38;5;28mrange\u001b[39m(start_frame, event.frames[\u001b[32m0\u001b[39m]), *event.frames, *\u001b[38;5;28mrange\u001b[39m(n, end_frame + \u001b[32m1\u001b[39m)]\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array([\u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m[t:(b + \u001b[32m1\u001b[39m), l:(r + \u001b[32m1\u001b[39m), :] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m frames])\n",
      "\u001b[31mIndexError\u001b[39m: index 79 is out of bounds for axis 0 with size 79"
     ]
    }
   ],
   "source": [
    "#event shape: (num_frames, _, _,channels)\n",
    "events_cropped_vids = [] \n",
    "for i in range(10):\n",
    "    event = get_event(eventspace[i], fullres_video[], nlevels=4)\n",
    "    events_cropped_vids.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event_vid in events_cropped_vids:\n",
    "    display_event(event_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for cv2 optical flow usage from:\n",
    "#1) https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html\n",
    "#2) https://www.geeksforgeeks.org/python-opencv-dense-optical-flow/\n",
    "\n",
    "firstframe = event[0]\n",
    "prev_gray = cv2.cvtColor(firstframe, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(firstframe)\n",
    "hsv_mask[..., 1] = 255\n",
    "\n",
    "flow_vid = []\n",
    "\n",
    "#write as loop for testing. Then actually make loop run longer than one frame\n",
    "#TODO change to not skip first frame??\n",
    "for frame in event[1:]:\n",
    "    curr_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None,\n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "    hsv_mask[..., 0] = angle * 180 / np.pi / 2\n",
    "\n",
    "    hsv_mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)\n",
    "    flow_vid.append(rgb)\n",
    "\n",
    "    prev_gray = curr_gray\n",
    "\n",
    "flow_vid = np.array(flow_vid)\n",
    "display_event(flow_vid)\n",
    "#display_event(fullres_video)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
